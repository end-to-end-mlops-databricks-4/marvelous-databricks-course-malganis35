resources:
  jobs:

    #------------------------------------------------------------------------
    # WEEKLY RUN (TO TRAIN AND IMPROVE MODEL)
    #------------------------------------------------------------------------

    weekly_training:
      name: ${bundle.name}-weekly-training
      # email_notifications: # in case of failure, can send to teams, slack, emails
      #     on_failure:
      #       - example@example.com
      schedule:
        quartz_cron_expression: "0 0 6 ? * MON"
        timezone_id: "Europe/Paris"
        pause_status: ${var.schedule_pause_status}
      tags: # interesting to group project / ressources to track costs, etc.
        project_name: "hotel-reservation-caotrido"
        job_type: "weekly_training"
        environment: "${var.env}"
        model_name: "basic_hotel_reservation_model"
        finops_cost_center: "mlops-cohort4"
        purpose: "ml-training"
      job_clusters:
        - job_cluster_key: "hotel-reservation-cluster"
          new_cluster:
            spark_version: "16.4.x-scala2.12"
            data_security_mode: "SINGLE_USER"
            node_type_id: "r3.xlarge"
            driver_node_type_id: "r3.xlarge"
            autoscale:
              min_workers: 1
              max_workers: 1
            spark_env_vars:
              "TOKEN_STATUS_CHECK": "{{secrets/mlops_course/git_token_status_check}}" # not needed if there is no integration test. For example, variable in .env file

      tasks:
        - task_key: "preprocessing"
          # job_cluster_key: "house-price-cluster"
          existing_cluster_id: ${target.cluster_id} # "0925-180914-5nzxv7hw"
          # job_cluster_key: "hotel-reservation-cluster"
          spark_python_task:
            python_file: "../scripts/01.process_new_data.py"
            parameters:
              - "--root_path"
              - "${workspace.root_path}"
              - "--env"
              - "${var.env}"
              - "--is_test"
              - "${var.is_test}"
              - "--write_mode"
              - "${var.write_mode}"
          libraries:
           - whl: ../dist/*.whl #custom libraries that will be installed in the cluster
        - task_key: "train_model"
          # job_cluster_key: "house-price-cluster"
          existing_cluster_id: ${target.cluster_id} # "0925-180914-5nzxv7hw"
          depends_on:
            - task_key: "preprocessing"
          spark_python_task:
            python_file: "../scripts/02.train_register_model.py"
            parameters:
              - "--root_path"
              - "${workspace.root_path}"
              - "--env"
              - "${var.env}"
              - "--git_sha"
              - "${var.git_sha}"
              - "--job_run_id"
              - "{{job.run_id}}"
              - "--branch"
              - "${var.branch}"
              - "--is_test"
              - "${var.is_test}"
          libraries:
            - whl: ../dist/*.whl
        - task_key: model_updated
          condition_task:
            op: "EQUAL_TO"
            left: "{{tasks.train_model.values.model_updated}}"
            right: "1"
          depends_on:
            - task_key: "train_model"
        - task_key: "deploy_model"
          depends_on:
            - task_key: "model_updated"
              outcome: "true"
          # job_cluster_key: "house-price-cluster"
          existing_cluster_id: ${target.cluster_id} # "0925-180914-5nzxv7hw"
          spark_python_task:
            python_file: "../scripts/03.deploy_model_serving.py"
            parameters:
              - "--root_path"
              - "${workspace.root_path}"
              - "--env"
              - "${var.env}"
              - "--is_test"
              - "${var.is_test}"
              - "--model_version"
              - "${var.model_version}"
          libraries:
            - whl: ../dist/*.whl
        - task_key: post_commit_status_required
          condition_task:
            op: "EQUAL_TO"
            left: "${var.is_test}"
            right: "1"
          depends_on:
            - task_key: "deploy_model"
        - task_key: "post_commit_status" #only for integration test
          depends_on:
            - task_key: "post_commit_status_required"
              outcome: "true"
          # job_cluster_key: "house-price-cluster"
          existing_cluster_id: ${target.cluster_id} # "0925-180914-5nzxv7hw"
          spark_python_task:
            python_file: "../scripts/04.post_commit_status.py"
            parameters:
              - "post_commit_check"
              - "--job_run_id"
              - "{{job.run_id}}"
              - "--job_id"
              - "{{job.id}}"
              - "--git_sha"
              - "${var.git_sha}"
              - "--repo"
              - "${var.repo}"
              - "--org"
              - "${var.org}"
          libraries:
            - whl: ../dist/*.whl
