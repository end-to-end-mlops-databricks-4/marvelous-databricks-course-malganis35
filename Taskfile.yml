version: '3'
dotenv: [.env]

vars:
  PACKAGE_IMPORT_NAME: hotel_reservation
  PYTHON_VERSION: 3.12

tasks:
  install:
    desc: Install project stack dependencies (task, uv, devbox)
    cmds:
      - sudo apt install task
      - curl -LsSf https://astral.sh/uv/install.sh | sh
      - curl -fsSL https://get.jetify.com/devbox | bash
    silent: false

  create-venv:
    desc: Create a virtual environment
    cmds:
      - uv venv -p {{.PYTHON_VERSION}} .venv
    silent: false

  dev-install:
    desc: Sync project dependencies with optionals
    cmds:
      - rm -rf .venv/
      - uv sync --extra dev

  test-install:
    desc: Sync only test dependencies
    cmds:
        - uv sync --extra test

  demo:
    desc: Test on a demo script
    cmds:
      - task: dev-install
      - uv run notebooks/demo.py

  run_upload_data:
    desc: Test on a demo script
    cmds:
      - task: dev-install
      - uv run notebooks/utils/run_upload_data.py --env dev --env-file .env --config project_config.yml

  run_process_data:
    desc: Test on a pre-processing script
    cmds:
      - task: dev-install
      - uv run notebooks/process_data.py

  run_process_new_data:
    desc: Test on a pre-processing script
    cmds:
      - task: dev-install
      - uv run notebooks/process_new_data.py

  run_create_mlflow_workspace:
    desc: Create the directory on Databricks for MLflow experiments
    cmds:
      - task: dev-install
      - uv run notebooks/utils/run_create_mlflow_workspace.py

  run_train_register_basic_model:
    desc: Run a basic experiment to train, log and register a model
    cmds:
      - task: dev-install
      - uv run notebooks/train_register_basic_model.py --git_sha "abcd12345" "--job_run_id" local_test_run --branch dev

  run_train_register_custom_model:
    desc: Run a basic experiment to train, log and register a custom model (LR with output with probabilities)
    cmds:
      - task: dev-install
      - uv build .
      - uv run notebooks/train_register_custom_model.py --git_sha "abcd12345" "--job_run_id" local_test_run --branch dev

  run_train_register_fe_model:
    desc: Run an experiment to train, log and register a model with Feature Engineering lookup
    cmds:
      - task: dev-install
      - uv run notebooks/train_register_fe_model.py --git_sha "abcd12345" "--job_run_id" local_test_run --branch dev

  run_predict_basic_model:
    desc: Download locally the latest basic model from MLFlow Experiment tracker and make prediction
    cmds:
      - task: dev-install
      - uv run notebooks/predict_basic_model.py --git_sha "abcd12345" "--job_run_id" local_test_run --branch dev

  run_predict_custom_model:
    desc: Download locally the latest custom model from MLFlow Experiment tracker and make prediction
    cmds:
      - task: dev-install
      - uv run notebooks/predict_custom_model.py --git_sha "abcd12345" "--job_run_id" local_test_run --branch dev

  run_predict_fe_model:
    desc: Download locally the latest fe model from MLFlow Experiment tracker and make prediction
    cmds:
      - task: dev-install
      - uv run notebooks/predict_fe_model.py --git_sha "abcd12345" "--job_run_id" local_test_run --branch dev

  run_deploy_basic_model_serving:
    desc: Deploy a model serving endpoint in Databricks from the basic model
    cmds:
      - task: dev-install
      - uv run notebooks/deploy_basic_model_serving.py --branch dev

  run_deploy_custom_model_serving:
    desc: Deploy a model serving endpoint in Databricks from the custom model
    cmds:
      - task: dev-install
      - uv run notebooks/deploy_custom_model_serving.py --branch dev

  run_test_basic_model_serving:
    desc: Test the Basic deployed model serving endpoint in Databricks
    cmds:
      - task: dev-install
      - uv run tests/functional/basic_model_serving_call.py --branch dev

  run_test_custom_model_serving:
    desc: Test the Custom deployed model serving endpoint in Databricks
    cmds:
      - task: dev-install
      - uv run tests/functional/custom_model_serving_call.py --branch dev

  run_local_streamlit_app:
    desc: Run locally the streamlit app for testing purpose
    cmds:
      - task: dev-install
      - uv run --with streamlit streamlit run app/app.py

  deploy_streamlit_app:
    desc: Deploy the streamlit app for inference
    cmds:
      - task: dev-install
      - uv build
      - cp dist/*.whl app/hotel_reservation-latest.whl
      - sed -i '/\.whl/d' app/requirements.txt
      - databricks bundle deploy
      - databricks apps deploy hotel-reservation-caotrido-app

  lint:
    desc: Run pre-commit hooks
    cmds:
      - task: dev-install
      - uv run pre-commit run --all-files

  test:
    desc: Run unit test and coverage
    cmds:
      - task: test-install
      - uv run --extra test pytest
      - 'echo "Note: you can open the HTML report with: open _artifact/coverage_re/index.html"'

  qa-lines-count:
    desc: Count the number of lines in each *.py script (excluding .venv)
    cmds:
      - echo "--> Count the number of lines in each *.py script (excluding .venv)"
      - >
        find . -path ./.venv -prune -o -name '*.py' -exec wc -l {} \; |
        sort -n |
        awk '{printf "%4s %s\n", $1, $2}{s+=$1}END{print s}'

  pc:
    desc: Run pre-commit hooks
    cmds:
      - task: dev-install
      - uv run pre-commit run --all-files

  clean:
    desc: Clean all temporary files
    cmds:
      - find . -type f -name "*.py[co]" -delete
      - find . -type d -name "__pycache__" -delete
      - find . -name '*.egg-info'  -type d -exec rm -rf {} +
      - rm -rf .venv/
      - rm -rf _artifact/ .devbox/ .pytest_cache/ .ruff_cache/ node_modules/
      - rm .coverage package.json package-lock.json digest.txt zz_git-diff.txt

  clean-databricks:
    desc: Clean all data, schema and experiments in Databricks
    cmds:
      - task: dev-install
      - uv run notebooks/utils/run_cleanup_mlflow_experiments.py
      - uv run notebooks/utils/run_cleanup_data.py

  digest:
    desc: Make a git digest of the repository
    cmds:
      - uv run --with gitingest gitingest .

  databrickscfg:
    desc: Show the databrickscfg file
    cmds:
      - cat ~/.databrickscfg

  help:
    desc: Print all tasks defined in the Taskfile
    cmds:
      - task -l
    silent: true

  default:
    cmds:
      - task: help
    silent: true
